{"pages":[],"posts":[{"title":"三篇图像动画论文的比较(FOMM,AA,SDEMT)","text":"FOMM和AA是很经典的论文，都来自作者Aliaksandr Siarohin，最近有一篇新论文在此基础上做了较大的改进，给出的效果看起来也很好，因此总结三个论文的内容。 FOMM AA Self-appearance-aided Differential Evolution for Motion Transfer FOMMmotion representation包含稀疏动作估计和稠密动作估计。 稀疏动作使用$\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)$来计算D中每个关键点处到S的仿射变换，具体方法是用泰勒展开，然后将涉及到$\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)$之外的变化都用$\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)$来代替。 \\mathcal{T_{S\\leftarrow D}}(z) \\approx \\mathcal{T_{S\\leftarrow R}}(p_k) + J_k(z-\\mathcal{T_{D\\leftarrow R}}(p_k))稠密动作用$\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)$的高斯分布之差计算热力图来找出变换发生的位置。对S进行稀疏动作的逆变换，得到K个结果(每个关键点一个变换)。然后将热力图和S变换结果一起喂给U-net，预测K+1个mask作为每个关键点上稀疏动作的权值，多出的一个mask用于处理背景。稠密动作就是各个关键点的稀疏动作的加权和。 extraction使用u-net分别提取参考帧到两个输入图像的仿射变换$\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)$，每个关键点对应一个变换。还额外输出一个遮挡图的通道。 transfer用稠密动作warp输入图像，然后用遮挡图计算阿达马乘积，最后通过一个解码器处理得到最终结果。 Articulated Animationmotion representation包含稀疏动作估计和稠密动作估计。论文和代码里热力图的概念被重复用在两个地方，一个是指输入图像提取出来的热力图，相当于特征点，另一个是稠密动作里用高斯分布得到的热力图，用于找出发生变换的位置。 稀疏动作使用PCA处理热力图的协方差得到仿射变换的线性变换的部分，热力图的均值作为仿射变换的偏移量，总共5个自由度，加上背景的恒等变换就得到稀疏动作。 稠密动作和FOMM的做法完全一样（包括使用的神经网络结构），稠密动作就是各个关键点的稀疏动作的加权和。 extraction使用U-net结构提取输入图像的特征图，经过一层卷积和softmax，得到热力图。 transfer和FOMM一样。 Self-appearance-aided Differential Evolution for Motion Transfermotion representation计算关键点的差值作为神经网络的输入，回归出系数来加权关键点的差值作为稀疏动作，同时作为微分方程 \\frac{d \\mathscr{T}}{d t}=\\mathscr{F}_{E}\\left(\\mathscr{T}^{(t)}, t\\right), \\quad \\text { for } t \\in[0,1]的初值$\\mathscr{F}^{(0)}$：，两侧积分就得到稠密动作。 extraction用编码器解码器网络获得S和D的关键点。 transfer 用特征提取网络提取输入图像的特征$\\mathbb{F}_{\\mathbb{S}}$。 将$\\mathbb{F}_{\\mathbb{S}}$用稠密动作进行扭曲得到$\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}$。 扭曲后用$\\mathscr{F}_A$来预测self-appearance流变形场$\\mathscr{T}_{App}$，$\\mathscr{T}_{App}$是用来扭曲原区域到缺失区域的（流）特征的。 计算$\\mathbb{F}_{A p p}=\\mathscr{T}_{A p p} \\circ \\widetilde{\\mathbb{F}}_{\\mathbb{S D}}$。 用生成器$\\mathscr{F}_G$为N个不同视角的view输出置信度掩码$C^{(j)}$，每个视角的置信度掩码各自进行归一化：$\\widetilde{C}^{(j)}(\\mathbf{x})=C^{(j)}(\\mathbf{x}) / \\sum_{j=0}^{N} C^{(j)}(\\mathbf{x})$。 用置信度掩码对$\\mathbb{F}_{A p p} ,\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}$分别计算加权和： \\overline{\\mathbb{F}}_{A p p}=\\sum_{j=1}^{N} \\widetilde{C}^{(j)} \\mathbb{F}_{A p p}^{(j)}, \\quad \\overline{\\widetilde{\\mathbb{F}}}_{\\mathbb{D}}=\\sum_{j=1}^{N} \\widetilde{C}^{(j)} \\widetilde{\\mathbb{F}}_\\mathbb{SD}^{(j)} 只有一个视图时，用$\\mathbb{F}_{A p p} ,\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}$连接起来喂给$\\mathscr{F}_G$的编码器部分用于生成图像。有多个时，用加权后的连接起来喂给$\\mathscr{F}_G$的编码器部分用于生成图像。 总结差别 FOMM和Articulated Animation是同一个人的工作，只有稀疏动作表示是不一样的，提取特征里后者只是多了生成热力图的步骤，Articulated Animation简化了计算稀疏动作的过程，用PCA方法和热力图的均值直接获得仿射变换，非常简洁。 Self-appearance-aided Differential Evolution for Motion Transfer使用解微分方程的方法计算动作，原因是neural-ODEs已被证明能够捕捉复杂的变换，可微动作演化可以泛化稀疏动作的预测（我觉得意思是不会因为不同对象或者数据集效果差别很大），同时避免雅克比矩阵和SVD的大量计算。 优点 Articulated Animation认为PCA可以更好的描述动作，所以用PCA来获得仿射变换矩阵的线性部分的参数。 Self-appearance-aided Differential Evolution for Motion Transfer 里指出它的方法泛化了FOMM，AA，Monkey-Net方法。 Self-appearance-aided Differential Evolution for Motion Transfer 在CSIM上与FOMM,AA拉开很大的距离。即该方法相比FOMM,AA能更好的保留原图像的特征。 Self-appearance-aided Differential Evolution for Motion Transfer 泛化能力很好，在A训练集上训练后，在B训练集上依然效果很好。比如卡通动画生成中，该论文的方法能很好的保留个体特征，而FOMM和AA很差。 缺点 FOMM在提取稀疏动作的时候用了泰勒展开，非常繁琐。 Articulated Animation里指出FOMM这类用关键点的方法在处理物体边界内的动作会出现不真实的效果。 Articulated Animation认为该论文的方法的泛化能力较弱，生成非活物的动画有难度，也就是说生成素描动画效果不会很好。 Self-appearance-aided Differential Evolution for Motion Transfer的网络结构太大，训练时间久。","link":"/2021/12/14/FOMM-AA-SDEMT/"},{"title":"JOKR笔记","text":"JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting摘要原视频和目标视频形状不同时，之前专注于特定对象先验的方法就会失败，作者提出联合关键点表达可以捕捉原视频和目标视频都有的动作，而且不需要物体先验或者数据采集。使用domain confusion项有利于对于两个domain的动作的一致的部分的解耦，可区分的外观和动作使得捕捉其中一个视频动作同时描绘另一个视频的风格的视频得以生成。 为处理物体有不同比例缩放或者不同方向的情形，作者应用了JOKR之间的仿射变换。这使得表达具有仿射不变性。 方法目标视频使用的外观来自视频A，动作来自视频B，共$N_a$和$N_b$帧。分割图$s_{a,i},s_{b,i}$是给定的数据，或者是通过现成的图像分割网络得到。 形状不变表达使用JOKR作为瓶颈。用无监督关键点提取器E，提取K个关键点$k_{a,i}$，提取器采用U-Net的做法，提取热力图$h_{a,i}$（a代表视频，i代表帧序号）来确定关键点位置。 为做到几何和外观的解耦，生成过程有两步： 给定$h_{a,i}$，生成器$G_{A}$被训练来输出一个同时对应提取到的关键点和物体形状的剪影。为减少参数数量，$G_{A},G_{B}$共用权重，除了最后一层。类似的，E也被用于两个视频。给定帧$a_i,b_j$，生成的剪影要最小化这个MSE loss。 \\mathcal{L}_{seg}=\\sum_{i=0}^{N_A-1}||G_A(E(a_i))-s_{a,i}||_2+\\sum_{j=0}^{N_B-1}||G_B(E(b_i))-s_{b,j}||_2训练生成器$R_A,R_B$来转换得到的分割图到原图上，因此添加了纹理。重建和感知损失为： \\begin{gathered} \\mathcal{L}_{L 1}=\\sum_{i=0}^{N_{A}-1}\\left\\|R_{A}\\left(G_{A}\\left(E\\left(a_{i}\\right)\\right)\\right)-a_{i}\\right\\|_{1}+\\sum_{j=0}^{N_{B}-1}\\left\\|R_{A}\\left(G_{B}\\left(E\\left(b_{j}\\right)\\right)\\right)-b_{j}\\right\\|_{1} \\\\ \\mathcal{L}_{\\text {LPIPS }}=\\sum_{i=0}^{N_{A}-1}\\left\\|\\mathcal{F}\\left(R_{A}\\left(G_{A}\\left(E\\left(a_{i}\\right)\\right)\\right)\\right)-\\mathcal{F}\\left(a_{i}\\right)\\right\\|_{2}+\\sum_{j=0}^{N_{B}-1}\\left\\|\\mathcal{F}\\left(R_{A}\\left(G_{B}\\left(E\\left(b_{j}\\right)\\right)\\right)\\right)-\\mathcal{F}\\left(b_{j}\\right)\\right\\|_{2} \\end{gathered}$\\mathcal{F}$是特征提取器。 共享表达AB视频描绘的物体可能来自不同domain，所以提取的关键点对于各自的视频可能有不同的语义信息。因此作者强制编码的关键点来自共享的分布，从而鼓励关键点捕捉同时来自两个视频的动作。AB的特定风格在生成器的权重里编码。为施加共享的分布，作者使用domain confusion loss。 此外还用一个判别器来区分A和B的domain的关键点，编码器被训练来欺骗他。 \\mathcal{L}_{\\mathrm{DC}}=\\sum_{i=0}^{N_{A}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{a, i}\\right), 1\\right)+\\sum_{j=0}^{N_{B}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{b, j}\\right), 1\\right)$\\ell_{bce}=-(q\\log(p)+(1-q)\\log(1-p))$是二元交叉熵损失函数。 关键点提取器尝试使得关键点分布无法区分，同时判别器做对抗它的训练： \\mathcal{L}_{\\mathrm{D}}=\\sum_{i=0}^{N_{A}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{a, i}\\right), 0\\right)+\\sum_{j=0}^{N_{B}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{b, j}\\right), 1\\right)时间连贯性为保证生成的视频是时间连贯的，即生成的动作平滑无抖动。应用时间正则化在生成的关键点上，最小化相邻帧之间的关键点的距离。 \\mathcal{L}_{\\mathrm{tmp}}=\\sum_{i=0}^{N_{A}-1}\\left\\|k_{a, i}-k_{a, i+1}\\right\\|_{2}+\\sum_{j=0}^{N_{B}-1}\\left\\|k_{b, j}-k_{b, j+1}\\right\\|_{2}因为物体有大动作的时候关键点的含义可能会变化（比如由后腿变成尾巴），所以作者应用随机仿射变换，比较变换后的关键点和变换后的图像提取出的关键点，来保证生成的关键点对任意仿射变换是等变的（equivariant）。即保证了每个关键点语义的一致性，对于一个仿射变换T，等变loss的公式是： \\mathcal{L}_{\\mathrm{eq}}=\\sum_{i=0}^{N_{A}-1}\\left\\|T\\left(E\\left(a_{i}\\right)\\right)-E\\left(T\\left(a_{i}\\right)\\right)\\right\\|_{1}+\\sum_{j=0}^{N_{B}-1}\\left\\|T\\left(E\\left(b_{j}\\right)\\right)-E\\left(T\\left(b_{j}\\right)\\right)\\right\\|_{1}关键点正则化关键点们可能会缩成一个点，因此有额外的两个损失项。 首先惩罚两个过近的关键点： \\mathcal{L}_{\\mathrm{sep}}=\\frac{1}{K^{2}} \\sum_{\\ell=0}^{K-1} \\sum_{\\ell \\neq r}\\left(\\sum_{i=0}^{N_{A}-1} \\max \\left(0, \\delta-\\left\\|k_{a, i}^{\\ell}-k_{a, i}^{r}\\right\\|^{2}\\right)+\\sum_{j=0}^{N_{B}-1} \\max \\left(0, \\delta-\\left\\|k_{b, j}^{\\ell}-k_{b, j}^{r}\\right\\|^{2}\\right)\\right)然后用剪影的损失鼓励关键点待在物体上： \\mathcal{L}_{\\mathrm{sill}}=\\frac{1}{K} \\sum_{\\ell=0}^{K-1}\\left(\\sum_{i=0}^{N_{A}-1}-\\log \\sum_{u, v} s_{a, i}(u, v) H_{a, i}^{\\ell}(u, v)+\\sum_{j=0}^{N_{B}-1}-\\log \\sum_{u, v} s_{b, j}(u, v) H_{b, j}^{\\ell}(u, v)\\right)两步优化大部分损失和形状有关，和纹理无关，所以可以用两步优化目标函数。首先用$\\mathcal{L}_D$训练判别器，同时训练E，$G_A,G_B$。第二步训练$R_A,R_B$。 增强数据有限时可能导致mode collapse。所以作者用随机仿射变换进行增强。因为保留背景是必要的，而这些增强可能会给生成的帧带来伪影（artifact）,所以这些增强直接作用在关键点上，在其传给判别器之前。 预测 得到E,T,G,R之后，用下面的公式得到结果： a b_{j}=R_{A}\\left(G_{A}\\left(T\\left(E\\left(b_{j}\\right)\\right)\\right)\\right)即有a的外观b的动作的第j帧。","link":"/2021/12/14/JOKR-report/"},{"title":"Hello World","text":"是时候建个blog了，参考的2021最全hexo搭建博客+matery美化+使用，写的比较详细。踩的坑都在main和master分支上。以后大概主要写学习相关的文章，记录一下免得忘记。顺便记录一下config的设置https://hexo.io/zh-cn/docs/configuration.html显示latex公式需要修改一些包，参考：成功解决在hexo中无法显示数学公式的问题","link":"/2021/12/12/hello-world/"},{"title":"在latex项目里用plotneuralnet","text":"plotneuralnet是一个绘制神经网络的工具，使用该工具可以将python脚本转换为latex脚本，编译得到pdf。需要以下几个步骤： 下载plotneuralnet的github源码，https://github.com/HarisIqbal88/PlotNeuralNet 找一个latex环境，可以是win/linux/overleaf，前两种需要安装latex的环境，比如texlive，可以参考https://github.com/luanshiyinyang/PlotNeuralNet，texlive在这里可以找到https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/。 写好脚本my_arch.py（名字可以随便起）放在源码的pyexamples文件夹下，编写的教程可以在其他地方搜到。 直接用python运行python my_arch.py，可以看到目录下的my_arch.tex 这样就得到了一个能够直接编译的tex文件。但是实际上很多情况下我们想把这个放进自己的项目里。我以一个比较大的tex项目为例，文件夹结构大致如下： 查看文件夹结构 . ├── Makefile ├── README.md ├── bibtex-style │ ├── gbt7714-2005.bst │ ├── thesis.bst │ └── thesis2.bst ├── code │ └── demo.cpp ├── ctex-fontset-adobe2.def ├── docs │ ├── abstract.tex │ ├── ack.tex │ ├── appendix1.tex │ ├── chap01.tex │ ├── chap02.tex │ ├── chap03.tex │ ├── chap04.tex │ ├── chap05.tex │ ├── chap06.tex │ ├── disclaim.tex │ ├── grading.tex │ ├── info.tex │ ├── progress.tex │ └── proposal.tex ├── fonts │ └── init_fonts.sh ├── gulpfile.js ├── image │ ├── appendix1 │ ├── chap03 │ │ ├── overleaf-config.jpg │ │ ├── overleaf-create-proj.jpg │ │ ├── overleaf-example.jpg │ │ ├── overleaf-upload-proj.jpg │ │ └── vscode-example.png │ ├── chap04 │ │ ├── example │ │ │ ├── 2007_000799.jpg │ │ └── result │ │ ├── compare │ │ │ └── zoom_dog.png │ │ └── error │ │ └── p_2008_001580.png │ └── template │ ├── readme.md │ └── logo.pdf ├── main.bib ├── main.pdf ├── main.tex ├── package.json ├── packages │ ├── algorithm2e.sty │ ├── algorithm2e.tex │ └── ctex-xecjk-adobefonts.def │ └── thubeamer.sty ├── code.sty └── thesis.cls 实际上只需要关心docs文件夹和main.tex，因为神经网络结构一般放在正文里。 以放入docs的chap03.tex为例，需要以下步骤： 将源码的layers文件夹放进自己的项目文件夹。 把my_arch.tex中这几行的内容（里面的配置可以调整）放入main.tex（最外层的文档）的序言（preamble）区，也就是放\\usepackage的区域里。 123\\documentclass[border=8pt, multi, tikz]{standalone} \\usepackage{import}\\subimport{../layers/}{init} 调整subimport里面的layers的路径，按照第一步来做，则应该改成\\subimport{layers/}{init}。 接下来将神经网络结构放入正文，把my_arch.tex里除上面这几行之外的内容完整地拷贝到chap03.tex里自己想放的位置。要去掉\\begin{document}和\\end{document}。 如果代码里有includegraphics，要把对应的图片放在对应的文件夹下。 编译就可以看到结果。 这样做会看到神经网络图片非常大，超出了页面。因为这个模块是tikzpicture的，所以可以用TikZ的方法来调整。 以整体缩放为例，参考在 LaTeX 中同步缩放 TikZ 与其中的 node，可以在\\begin{tikzpicture}上面加入下面这段代码： 12345\\tikzset{global scale/.style={ scale=#1, every node/.append style={scale=#1} }} 然后把\\begin{tikzpicture}改成\\begin{tikzpicture}[scale=0.5]，就能把整个大小缩放为原来的0.5倍。","link":"/2022/03/17/plotneuralnet-in-a-project/"}],"tags":[{"name":"图像动画","slug":"图像动画","link":"/tags/%E5%9B%BE%E5%83%8F%E5%8A%A8%E7%94%BB/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"latex","slug":"latex","link":"/tags/latex/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"}],"categories":[]}